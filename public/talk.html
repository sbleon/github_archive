<!DOCTYPE html>
<html>
  <head>
    <title>Roll-your-own Materialized Views</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-notes {
        font-size: 20px;
      }
      .remark-notes li {
        margin-bottom: 0.5em;
      }
      .remark-slide-content {
        font-size: 25px;
      }
      .remark-slide-content ul {
        margin-top: 0.5em;
      }
      .remark-slide-content li {
        margin-bottom: 0.5em;
      }
      .remark-slide-content h3 {
        margin-bottom: 0.2em;
      }
    </style>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Roll-your-own Materialized Views
## (for fun and profit!)

Leon Miller-Out
CTO, Singlebrook

---
## What's your database experience?
???
Show of hands
---
## What's your database experience?

Who has...

- built a relational-database-backed web app?

???
web app
---
## What's your database experience?

Who has...

- built a relational-database-backed web app?
- written some SQL?

???
SQL
---
## What's your database experience?

Who has...

- built a relational-database-backed web app?
- written some SQL?
- written a JOIN?

???
JOIN
---
## What's your database experience?

Who has...

- built a relational-database-backed web app?
- written some SQL?
- written a JOIN?
- created a database view?

???
DB view
---
## What's your database experience?

Who has...

- built a relational-database-backed web app?
- written some SQL?
- written a JOIN?
- created a database view?
- created a trigger?

???
- Trigger
- You know what you need to know so you can leave now ;-)
---
## What's your database experience?

Who has...

- built a relational-database-backed web app?
- written some SQL?
- written a JOIN?
- created a database view?
- created a trigger?
- encountered performance issues in a web app?

???
Performance issues
---

## Why am I talking about this boring database stuff?
- Software rot: as your database grows, your queries will "naturally" slow down.
- Mitigating this decay is required when maintaining a growing application.
- This is generally a good problem to have, because it usually means people are using your software!

???
- Have you ever had slow database queries slow down your application? If you haven't, you will someday. It's a kind of digital decay: dbs will slow down over time as the size grows.
- Unless you hate your users, that is!
---
## How do we fight against growth-induced database sluggishness?

- optimize your queries (change SQL, change indexes, etc)

???
- You have several options for dealing with this:
---
## How do we fight against growth-induced database sluggishness?

- optimize your queries (change SQL, change indexes, etc)
- throw money at it (more hardware)
---
## How do we fight against growth-induced database sluggishness?

- optimize your queries (change SQL, change indexes, etc)
- throw money at it (more hardware)
- change database infrastructure (sharding, replication, etc)

???
- Any or all of these options might be appropriate depending on your budget, the skillsets you're working with, your particular situation.
- Sometimes the slowness is caused by expensive joins (typically when merging large datasets from multiple tables). If this is your problem, materialized views may be the answer.
---
## How do we fight against growth-induced database sluggishness?

- optimize your queries (change SQL, change indexes, etc)
- throw money at it (more hardware)
- change database infrastructure (sharding, replication, etc)
- Pre-calculate using Matarialized Views
---
## What is a View?

- A stored SQL query that you can read from as if it were a table.
- Views can be seen as a kind of code re-use, helping to keep your SQL DRY (Don't Repeat Yourself)
- e.g.
  ```sql
  CREATE VIEW event_reports AS
  SELECT events.type, repos.name as repo_name, users.username
  FROM events
    INNER JOIN repos ON repos.id = events.repo_id
    INNER JOIN users ON users.id = events.user_id;

  SELECT * FROM event_reports LIMIT 1;

      type    |  username  |    repo_name
  ------------+------------+-----------------
   WatchEvent | Deovandski | bssthu/KSP_GPWS
  ```
---
## What is a Materialized View?

- A view (aka a stored query) that is stored (on disk) like a table.
???
- Materialized View = A view stored like a table.
---
## What is a Materialized View?

- A view (aka a query) that is stored (on disk) like a table.
- Your DB can JOIN, filter, and aggregate *once*, and **re-use the results**.

???
- Instead of joining all those rows every time you query, join them once, store the joined columns, and update it when necessary.
- A materialized view is like a view in that it is composed of data from tables (which can includes joins, filters, aggregates, etc). However, unlike a view, it is stored on disk, like a table.
---

class: center, middle

## Native Support?
### We are not using CREATE MATERIALIZED VIEW

???
## Native Support
- Your database engine may have some support for materialized views. E.g. Postgres has them, but there's one big pitfall. They must be regenerated all at once, instead of updating individual rows only when necessary. That does not work well for big tables. So for this talk, we will not be using that feature of Postgres.

---

## Overview

1. Create a view that pulls together the data
2. Create a table with the same rows as the view. This is your materialized view.
3. Create triggers that upsert the materialized view when the underlying tables' data changes.
4. UPDATE all of the rows to trigger the triggers.

---

class: center, middle

# Demo

http://localhost:3000/event_reports

???
- I put together a tiny Rails app to view Github event data.
- It's got all Github events for the first three days of this year, which is about 1.3M events records, and about 200K user and repo records. This is not a particularly large database, but should be big enough that JOINs take a bit of time to run.
- Search for repo "ra"
- Search for user "b"
- Always sorting by event timestamp.

---
## Data model

```
---------         ----------         ---------
| Users | <-----> | Events | <-----> | Repos |
--------- 1     * ---------- *     1 ---------
```

- Data from https://www.githubarchive.org/
- All events from 2016-01-01 - 2016-01-03
- 370MB compressed with gzip
- 1.3M events, 200k users, 250k repos
---
## Step 1: Create regular view

We're using a view to keep the query in the app very simple.

```sql
CREATE VIEW event_reports AS
 SELECT events.id,
    events.type,
    events.created_at,
    events.payload,
    repos.id AS repo_id,
    repos.name AS repo_name,
    repos.url AS repo_url,
    users.id AS user_id,
    users.username,
    users.url AS user_url,
    users.avatar_url
   FROM ((events
     JOIN repos ON ((repos.id = events.repo_id)))
     JOIN users ON ((users.id = events.user_id)));

SELECT * FROM event_reports WHERE [some filters];
```

???
- If you have a smaller amount of data, you could stop here and still get the
  benefits of simpler application code. Switching to a materialized view later
  will be (relatively) easy if you have this in place already.
- (In Rails, you can set up an ActiveRecord class that uses this view for its table. This can help to simplify your code by removing repeated joins, and maybe improve your memory usage by creating fewer AR objects. YMMV with other ORMs.)
---

## Why is it slow?

We must count the results in order to paginate, but counting is hard!

```sql
EXPLAIN SELECT COUNT(*) FROM "event_reports" WHERE (repo_name like 'ra%');
```
```
                                                   QUERY PLAN
-----------------------------------------------------------------------------------------------
Aggregate  (cost=126124.19..126124.20 rows=1 width=0)
  ->  Nested Loop  (cost=1521.50..126091.74 rows=12980 width=0)
    ->  Hash Join  (cost=1521.08..120106.78 rows=12980 width=4)
        Hash Cond: (events.repo_id = repos.id)
        ->  Seq Scan on events  (cost=0.00..113603.02 rows=1294102 width=8)
        ->  Hash  (cost=1489.87..1489.87 rows=2497 width=4)
          ->  Bitmap Heap Scan on repos  (cost=18.37..1489.87 rows=2497 width=4)
              Filter: ((name)::text ~~ 'ra%'::text)
              ->  Bitmap Index Scan on index_repos_on_name  (cost=0.00..17.75 rows=533 width=0)
                Index Cond: (((name)::text ~>=~ 'ra'::text) AND ((name)::text ~<~ 'rb'::text))
    ->  Index Only Scan using users_pkey on users  (cost=0.42..0.45 rows=1 width=4)
        Index Cond: (id = events.user_id)
```

???
- If I were to look at query plans to troubleshoot the poor performance of the original, non-materialized approach, I might find something like this.
- This is complex, and has a high estimated cost (126K hand-wavy units)
- Filtering by data in one table, sorting on data in another. This means the JOINs and filtering need to happen before the sorting, requiring loading of lots of data from multiple tables. If the JOINing has already happened, Postgres can load records from only one table.

---

## Step 2: Create materialized view

```sql
CREATE TABLE fast_event_reports (
    id bigint NOT NULL,
    type character varying NOT NULL,
    created_at timestamp without time zone NOT NULL,
    payload jsonb NOT NULL,
    repo_id integer NOT NULL,
    repo_name character varying NOT NULL,
    repo_url character varying NOT NULL,
    user_id integer NOT NULL,
    username character varying NOT NULL,
    user_url character varying NOT NULL,
    avatar_url character varying NOT NULL
);

ALTER TABLE fast_event_reports
ADD CONSTRAINT fk_fast_event_reports_events (id)
REFERENCES events (id) ON DELETE CASCADE ON UPDATE CASCADE;

ALTER TABLE fast_event_reports
ADD CONSTRAINT fk_fast_event_reports_repos (repo_id)
REFERENCES repos (id) ON DELETE CASCADE ON UPDATE CASCADE;

ALTER TABLE fast_event_reports
ADD CONSTRAINT fk_fast_event_reports_users (user_id)
REFERENCES users (id) ON DELETE CASCADE ON UPDATE CASCADE;
```
???
- This table will be empty. How do we fill it and keep it up to date?
---

## Step 3: Create triggers

```sql
CREATE FUNCTION fn_trig_upsert_event() RETURNS trigger
  LANGUAGE plpgsql AS $$
BEGIN
  INSERT INTO fast_event_reports(
    id, type, created_at, payload, repo_id, repo_name, repo_url,
    username, user_id, user_url, avatar_url)
  SELECT id, type, created_at, payload, repo_id, repo_name, repo_url,
    username, user_id, user_url, avatar_url
  FROM event_reports
  WHERE id = NEW.id

  ON CONFLICT(id) DO UPDATE
  SET type = EXCLUDED.type,
      created_at = EXCLUDED.created_at,
      payload = EXCLUDED.payload;

  RETURN null;
END
$$;

CREATE TRIGGER trig_upsert_event
AFTER INSERT OR UPDATE ON events
FOR EACH ROW EXECUTE PROCEDURE fn_trig_upsert_event();
```

Create a similar function for each table.

???
- We only need to handle INSERTs and UPDATEs because DELETEs are cascaded via foreign keys.
- This uses a slick new feature of Postgres 9.5 "ON CONFLICT DO UPDATE" that does conflict-free upserts! See your RDBMS' documentation to figure out the best way to upsert in it.
- All of the triggers pull data from the view, so the JOINs do not need to be repeated in each trigger.
- I think that all triggers could be handled by a function that creates and runs SQL, but I haven't tried this yet.

---

## Step 4: UPDATE all rows to trigger the triggers, populating the materialized view

```sql
UPDATE events
SET id = id;
```

If you have a busy database, consider doing this in batches to avoid needing a table lock.

???
- That is a silly-looking query!
---

## Step 5: Profit!

Benchmark grabbing 100 records from a random offset, 3 times each.
```
            user     system      total        real
view    0.020000   0.000000   0.020000 ( 32.662143)
mview   0.010000   0.010000   0.020000 (  3.311239)
```
This is roughly a 10x speed improvement!
---

### Before
```sql
EXPLAIN SELECT * FROM event_reports ORDER BY created_at
LIMIT 100 OFFSET 9710;

Limit  (cost=11503.78..11622.24 rows=100 width=752)
  ->  Nested Loop  (cost=1.27..1533000.72 rows=1294102 width=752)
    ->  Nested Loop  (cost=0.85..936302.07 rows=1294102 width=656)
        ->  Index Scan using index_events_on_created_at on events  (cost=0.43..338039.43 rows=1294102 width=581)
        ->  Index Scan using repos_pkey on repos  (cost=0.42..0.45 rows=1 width=79)
          Index Cond: (id = events.repo_id)
    ->  Index Scan using users_pkey on users  (cost=0.42..0.45 rows=1 width=100)
        Index Cond: (id = events.user_id)
```
---
### Before
```sql
EXPLAIN SELECT * FROM event_reports ORDER BY created_at
LIMIT 100 OFFSET 9710;

Limit  (cost=11503.78..11622.24 rows=100 width=752)
  ->  Nested Loop  (cost=1.27..1533000.72 rows=1294102 width=752)
    ->  Nested Loop  (cost=0.85..936302.07 rows=1294102 width=656)
        ->  Index Scan using index_events_on_created_at on events  (cost=0.43..338039.43 rows=1294102 width=581)
        ->  Index Scan using repos_pkey on repos  (cost=0.42..0.45 rows=1 width=79)
          Index Cond: (id = events.repo_id)
    ->  Index Scan using users_pkey on users  (cost=0.42..0.45 rows=1 width=100)
        Index Cond: (id = events.user_id)
```
### After
```sql
EXPLAIN SELECT * FROM fast_event_reports ORDER BY created_at
LIMIT 100 OFFSET 9710;

Limit  (cost=1698.43..1715.92 rows=100 width=728)
  ->  Index Scan using index_fast_event_reports_on_created_at on fast_event_reports  (cost=0.43..226301.87 rows=1294102 width=728)
```
---
### Pros
- This can speed up queries. What kinds of queries can benefit?
  - Those that join multiple large tables
  - Those using ORDER BY and WHERE clause
  - Those that handle a large number of rows
- Get more performance with current database hardware
- Developers learn more about SQL

### Cons
- SQL code is relatively difficult to maintain.
  - E.g. adding a column is a lot of work!
    - Columns must to be added in multiple places
    - All triggers must be updated
- Developers need to know some SQL
---

class: center, middle

# Questions?

Contact me at leon@singlebrook.com or on Meetup.com!

https://github.com/sbleon/github_archive

???
- If anyone wants a copy of my Vagrant box to play around with this, I can make it available.

    </textarea>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
